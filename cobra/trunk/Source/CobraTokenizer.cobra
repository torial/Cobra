class CobraTokenizer
	inherits Tokenizer

	test
		t = CobraTokenizer()

	var _indentCount as int
	var _substLBracketCount as int
	var _inSubstStringSingle = false

	# Note: The Tokenizer class handles it's input one line at a time,
	#       and retains the \n at the end of the line. This affects
	#       the regex design for the tokens.

	get orderedTokenSpecs as List<of String> is override
		return [
			# whitespace
			r'BLANK_TABS_LINE_1		^[\t]+$',
			r'BLANK_TABS_LINE_2		^[\t]+\#.*$',
			r'INDENT_MIXED_1		^[\t]+[ ]+',
			r'INDENT_MIXED_2		^[ ]+[\t]+',
			r'INDENT_ALL_TABS		^[\t]+',
			r'NO_INDENT				^(?!\t|\n|#)',
			r'EOL					\n',
			r'SINGLE_LINE_COMMENT	\#.*',
			r'SPACE					[ \t]+',

			r'OPEN_GENERIC			[A-Za-z_][A-Za-z0-9_]*<of[ \n\r\t]',
			r'OPEN_IF				if\(',
			r'OPEN_CALL				[A-Za-z_][A-Za-z0-9_]*\(',

			r'FLOAT_LIT				\d[\d_]*\.\d+f',
			r'DECIMAL_LIT			\d[\d_]*\.\d+d?',
			r'INTEGER_LIT			\d[\d_]*',

			r'INT_SIZE				int[0-9]+',
			r'UINT_SIZE				uint[0-9]+',
			r'FLOAT_SIZE			float[0-9]+',

			r"CHAR_LIT_SINGLE		c'\\?.'",
			r'CHAR_LIT_DOUBLE		c"\\?."',

			# doc strings
			r'DOC_STRING_START		"""\n',

			# substituted strings
			r'RBRACKET_SPECIAL		]',
			r"STRING_START_SINGLE	'[^'\n\[]*\[",
			r"STRING_PART_SINGLE	\][^'\n\[]*\[",
			r"STRING_STOP_SINGLE	\][^'\n\[]*'",

			r'STRING_START_DOUBLE	"[^"\n\[]*\[',
			r'STRING_PART_DOUBLE	\][^"\n\[]*\[',
			r'STRING_STOP_DOUBLE	\][^"\n\[]*"',

			r'STRING_PART_FORMAT	:[^X"\n\[]*(?=])'.replace('X', "'"),

			# plain strings
			r"STRING_NOSUB_SINGLE	ns'[^'\n]*'",
			r'STRING_NOSUB_DOUBLE	ns"[^"\n]*"',

			r"STRING_SINGLE			'[^'\n]*'",
			r'STRING_DOUBLE			"[^"\n]*"',

			r'TOQ					to\?',
			r'ID					[A-Za-z_][A-Za-z0-9_]*',
		]

	get unorderedTokenSpecs as List<of String> is override
		return [
			r'DOT				\.',
			r'DOTDOT			\.\.',
			r'COLON				:',
			r'PLUS				\+',
			r'PLUSPLUS			\+\+',
			r'MINUSMINUS		\-\-',
			r'MINUS				-',
			r'STARSTAR			\*\*',
			r'STAR				\*',
			r'SLASHSLASH		//',
			r'SLASH				/',
			r'PERCENT			%',
			r'ASSIGN			=',
			r'LPAREN			\(',
			r'RPAREN			\)',
			r'LBRACKET			\[',
			r'RBRACKET			\]',
			r'LCURLY			\{',
			r'RCURLY			\}',
			r'SEMI				;',
			r'COMMA				,',
			r'OBJECT_OPEN		<<',
			r'OBJECT_CLOSE		>>',
			r'DICT_OPEN			{',
			r'DICT_CLOSE		}',
			r'QUESTION			\?',
			r'BANG				\!',

			r'EQ				==',
			r'NE				<>',
			r'LT				<',
			r'GT				>',
			r'LE				<=',
			r'GE				>=',

			r'PLUS_EQUALS		\+=',
			r'MINUS_EQUALS		\-=',
			r'STAR_EQUALS		\*=',
			r'SLASH_EQUALS		\/=',
			r'PERCENT_EQUALS	%=',
			r'QUESTION_EQUALS	\?=',
			r'BANG_EQUALS		\!=',
		]

	get keywords as String is override
		# CC: this should be multiline string with comments. See CobraTokenizer.py
		return 'use import namespace enum class inherits implements interface struct where must be callable cue def as get set pro var from code test shared virtual override ref vari require ensure old this base is assert branch on expect if else using while post for down to step break continue try catch success finally throw except event listen ignore raise and or not implies every all to to\\? pass print stop return in bool char int decimal float of passthrough true false nil dynamic same instance'

	def _reset is override
		base._reset()
		_indentCount = 0
		_substLBracketCount = 0

	def afterStart is override
		base.afterStart()
		# CC:
		# _tokenDefsByWhich['STRING_PART_SINGLE'].isActiveCall = lambda tokenizer: tokenizer.inSubstStringSingle
		# _tokenDefsByWhich['STRING_STOP_SINGLE'].isActiveCall = lambda tokenizer: tokenizer.inSubstStringSingle
		for which as String in 'RBRACKET_SPECIAL STRING_PART_SINGLE STRING_STOP_SINGLE STRING_PART_DOUBLE STRING_STOP_DOUBLE'.split(c' ')  # CC: remove "as String"
			_tokenDefsByWhich[which].isActive = false

	def isActiveCall(tok as IToken) as bool
		if tok.which=='STRING_PART_SINGLE' or tok.which=='STRING_STOP_SINGLE'
			return _inSubstStringSingle
		return true
