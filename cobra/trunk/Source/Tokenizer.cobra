"""
Tokenizer.py


Requirements of a nice tokenizer

	- Produce tokens on demand instead of all at once.

	- Consume text in lines so that text can be streamed in.

	- Accurately and automatically report for each token
		which kind, text, line number, column number, character index and length

	- Allow methods to intercept when a token is encountered in order to
		- modify it
		- skip it
		- replace it by a list of tokens

	- Execute unit tests even on importation to ensure quality.

TODO:

	- Convert all Python

	- Reorder in top down fashion

	- Go back and add test, require and ensure whereever possible

	- Get tests working

	- Check for TODO


from Common import *
from cStringIO import StringIO
from Stack import Stack
import re


class Token

			def construct(which, text, value, **values)
				.which = which
				.text = text
				.value = value
				.fileName = ''
				.lineNum = 0
				__dict__.update(values)

			@property
			def length
				return len(.text)

			def __str__
				s = .text
				if '\t' in s or '\n' in s or '\r' in s
					s = repr(s)[1-1]
				if s
					if s.lower()==.which.lower()  # if keyword...
						return '"%s"' % s
					else
						return '"%s" (%s)' % (s, .which)
				else
					return '"%s"' % .which  # INDENT, DEDENT, etc.

	def __repr__
		s = '%s(%r, %r, %r' % (__class__.__name__, .which, .text, .value)
		if hasattr('lineNum')
			s += ', %r' % .lineNum
		s += ')'
		return s

	def copy
		c = __class__(.which, .text, .value)  # TODO different way to do this?
		c.__dict__.update(__dict__)
		return c


			class TokenizerError(ValueError)
				```
				Raised by nextToken() when there are errors in the source trying to be tokenized.
				```
				def construct(lineNum, msg)
					ValueError.construct(msg)
					.msg = msg
					.lineNum = lineNum


class Tokenizer

	verbosity = 0
	willAlwaysEndWithNewLine = true

				def construct(**args)
					.didReset = false
					__dict__.update(args)
					_reset()
					_tokenDefsStack = Stack(TokenDefSet)

				def _reset
					.file = nil
					.readLine = nil
					.sourceLine = nil
					.lastToken = nil

					.lineNum = 0
					.colNum  = 0
					.charNum = 0

					_tokenQueue = []  # needed when token methods return lists of tokens
					_tokenMethods = {}
					_keywordToWhichToken = {}

					.didReset = true

				def startFileNamed(filename)
					.file = open(filename, 'rt')
					.afterStart()
					return self

				def startSource(source)
					.file = StringIO(source)
					.afterStart()
					return self

				def nextToken
					```
					Consumes a token and returns it, making it the .lastToken.
					Returns nil when there are no tokens left.
					```
					assert .readLine, 'Not started.'
					.lastToken = _nextToken()
					if .verbosity>=1
						print '<> nextToken() returning %r' % .lastToken
					return .lastToken

				def allTokens
					```
					Returns all remaining tokens as a list.
					```
					tokens = []
					while true
						t = .nextToken()
						if t is nil
							break
						tokens.append(t)
					return tokens

				def restart
					```
					After calling this, you can use the tokenizer anew.
					Normally you would call this after completing the tokenization of a source file.
					```
					.readLine = nil
					if .file
						.file.close()
						.file = nil
					_reset()


				def keywordOrWhich(tok, which='ID')
					```
					Changes the token to a keyword if it is one, otherwise sets its which to identifier.
					Returns the token.
					This self utility method is typically called by a subclass from the method for the "identifier" token.
					```
					tok.which = _keywordToWhichToken.get(tok.text, which)
					return tok

		# the following three attributes can be conveniently filled out in order to provide the
		# tokenizer with its needed TokenDefs. See CobraTokenizer for an example.
		orderedTokenSpecs = nil
		unorderedTokenSpecs = nil
		keywords = nil

		def afterStart
			```
			Sets up self attrs
				readLine as callable
				tokenDefs as list
			```
			assert .didReset, 'Never reset. Probably the subclass overrides _reset() but forgets to invoke base.'
			assert not .readLine, 'Already started.'
			.readLine = .file.readline
			# create a single list of all token defs in the correct order
			.tokenDefs = []
			if .orderedTokenSpecs
				for spec in .orderedTokenSpecs
					.addTokenSpec(spec)
			others = []
			if .unorderedTokenSpecs
				for spec in .unorderedTokenSpecs
					others.append(.tokenDefForSpec(spec))
			if .keywords
				keywords = .keywords
				if inherits(keywords, str)
					t = []
					for line in keywords.split('\n')
						line = line.strip()
						if not line or line.startswith('#')
							continue
						t.extend(line.split())
					keywords = t
				for word in keywords
					_keywordToWhichToken[word] = word.upper()
			others.sort(lambda a, b cmp(a.length, b.length))
			others.reverse()  # longest tokens need to be matched first
			.tokenDefs.extend(others)
			.pushTokenDefs(.tokenDefs)
			if 0
				i = 0
				print '<> tokenDefs'
				for tokenDef in .tokenDefs
					print '    [%i] %r' % (i, tokenDef)
					i += 1

					def addTokenSpec(spec)
						.tokenDefs.append(.tokenDefForSpec(spec))

					def tokenDefForSpec(spec)
						```
						Returns a TokenDef object for a spec such as
							r'ID			[A-Za-z_][A-Za-z0-9_]*'
						```
						spec = spec.strip().replace('\t', ' ')
						which, re = spec.split(' ', 1)
						re = re.strip()
						assert which
						assert re
						return TokenDef(which=which, regExSource=re)

				def _nextToken
					if _tokenQueue
						# eat up queue first
						tok = _tokenQueue[0]
						del _tokenQueue[0]
						return tok
					if not .sourceLine or .sourceLineIndex==len(.sourceLine)
						if not _obtainSource()
							return nil
					try
						for tokenDef in .tokenDefs
							if tokenDef.ignore  # TODO rename to ignoreCount
								tokenDef.ignore -= 1
								continue
							if not tokenDef.isActive
								continue
							if tokenDef.isActiveCall
								if not tokenDef.isActiveCall
									continue
							.curTokDef = tokenDef  # this enables onTOKENWHICH() methods to access the current tokenDef
							#print '<> Trying to match %r' % tokenDef
							match = tokenDef.match(.sourceLine, .sourceLineIndex)
							if match is nil
								#print '<> No match on %r for %r' % (tokenDef, .sourceLine[.sourceLineIndex])
								continue
							text = match.group()
							assert inherits(text, str)
							#print '<> Match! %r - %r' % (text, tokenDef)
							tok = Token(tokenDef.which, text, text, lineNum=.lineNum, colNum=.colNum, charNum=.charNum)
							.sourceLineIndex += len(text)
							.colNum += len(text)
							.charNum += len(text)

							# enable methods to customize handling of tokens
							reinvoke = false
							tokenMethod = _tokenMethods.get(tok.which, NotFound)
							if tokenMethod is NotFound
								tokenMethod = _tokenMethods[tok.which] = getattr('on'+tok.which, nil)
							if tokenMethod
								stuff = tokenMethod(tok)
								if stuff is nil
									# skip token, so go to the next one
									reinvoke = true
								elif inherits(stuff, Token)
									tok = stuff
								elif inherits(stuff, list)
									# handle inserting tokens into the stream of tokens
									_tokenQueue.extend(stuff)
									reinvoke = true

							# finished with current line?
							if .sourceLineIndex==len(.sourceLine)
								.lastSourceLine = .sourceLine
								.sourceLine = nil
								.sourceLineIndex = -1

							# handle token skipping
							if reinvoke
								tok = _nextToken()

							# yay!
							return tok
					finally
						.curTokenDef = nil

					# no match
					.error('Lexical error at %r' % .sourceLine[.sourceLineIndex])

	def _obtainSource
		.sourceLine = .readLine()
		if .sourceLine in (nil, '')
			# end of source
			return false
		numLines = .sourceLine.count('\n')
		if numLines==0
			if .willAlwaysEndWithNewLine
				.sourceLine += '\n'
		#print '<> sourceLine = %r' % .sourceLine
		if numLines
			if numLines==1
				assert .sourceLine.endswith('\n'), .sourceLine
			else
				assert false, 'Expecting readline() to return one line instead of many.'
		.sourceLineIndex = 0
		.lineNum += 1
		.colNum = 1
		return true

	def error(msg)
		raise TokenizerError(.lineNum, msg)

	def pushTokenDefs(defs)
		assert inherits(defs, list, TokenDef), defs
		defsByWhich = {}
		for tokenDef in defs
			assert not defsByWhich.has_key(tokenDef.which), tokenDef
			defsByWhich[tokenDef.which] = tokenDef
		_tokenDefsStack.push(TokenDefSet(defs, defsByWhich))
		.tokenDefs = defs
		.tokenDefsByWhich = defsByWhich

	def popTokenDefs
		_tokenDefsStack.pop()
		if _tokenDefsStack
			tokenDefSet = _tokenDefsStack.peek()
			defs = tokenDefSet.defs
			defsByWhich = tokenDefSet.defsByWhich
		else
			defs = defsByWhich = nil
		.tokenDefs = defs
		.tokenDefsByWhich = defsByWhich


			class TokenDefSet

				def construct(defs, defsByWhich)
					.defs = defs
					.defsByWhich = defsByWhich


			class TokenDef
				```
				Attrs/properties
					which
					regExSource
					re
					length
					ignore
					isActiveCall
				```

				def construct(which, regExSource)
					assert inherits(which, str)
					assert inherits(regExSource, str)
					.which = which
					.regExSource = regExSource
					.re = re.compile(regExSource)
					.length = len(regExSource)
					.ignore = 0
					.isActive = true
					.isActiveCall = nil

				def match(*args)
					return .re.match(*args)

				def __repr__
					return '%s(%s, %r, %s)' % (
						__class__.__name__, .which, .regExSource, .re)


class TestTokenizer(Tokenizer)

	orderedTokenSpecs = [
		r'OPEN_IF		ifx\(',
		r'ID			[A-Za-z_][A-Za-z0-9_]*',
		r'SPACE			[ ]+',
		r'NEWLINE		\n',
	]

	unorderedTokenSpecs = [
		r'DOT			\.',
		r'COLON			',
		r'PLUS			\+',
		r'ASSIGN		=',
		r'EQUALS		==',
	]

	keywords = '''
		# quality control
		assert

		# conditionals
		if else
	'''

	def _reset
		Tokenizer._reset
		.idCount = 0

	def onID(tok)
		assert tok
		.idCount += 1
		return tok

	def onSPACE(tok)
		return nil

	def onNEWLINE(tok)
		return tok

	def onDOT(tok)
		return [tok, tok.copy()]


def test()
	# basics
	t = TestTokenizer()
	t.startSource('hello there')
	tokens = t.allTokens()
	assert join(each(tokens,'which'))=='ID ID NEWLINE', tokens
	assert t.idCount==2

	# the tokenizer lets methods insert tokens
	t.restart()
	t.startSource('hello.there')
	tokens = t.allTokens()
	assert join(each(tokens,'which'))=='ID DOT DOT ID NEWLINE', tokens

	# tokens know their line numbers and columns and lengths
	t.restart()
	t.startSource('''hello
there
 you
 ''')
 	tokens = [tok for tok in t.allTokens() if tok.which!='NEWLINE']
 	assert len(tokens)==3, tokens
 	hello, there, you = tokens
 	assert hello.lineNum==1
 	assert hello.colNum==1
 	assert hello.length==5
 	assert there.lineNum==2
 	assert there.colNum==1
 	assert there.length==5
 	assert you.lineNum==3
 	assert you.colNum==2
 	assert you.length==3


def each(stuff, name)
	for x in stuff
		value = getattr(x, name)
		if iscallable(value)
			value = value()
		yield value

def join(stuff, sep=' ')
	return sep.join([str(x) for x in stuff])

"""

use System.Text.RegularExpressions

interface IToken

	pro which as String
	pro text as String

	get fileName as String
	get lineNum as int


class Token
	implements IToken

	var _fileName as String
	var _lineNum as int
	var _colNum as int
	var _charNum as int
	var _which as String
	var _text as String
	var _value as Object?  # TODO I think it would make more sense for this to be dynamic

	def construct(fileName as String, lineNum as int, colNum as int, charNum as int, which as String, text as String, value as Object?)
		_fileName = fileName
		_lineNum = lineNum
		_colNum = colNum
		_charNum = charNum
		_which = which
		_text = text
		_value = value

	pro which from var

	pro text from var

	get length as int
		return _text.length

	get fileName from var

	get lineNum from var

	def toString as String is override
		test
			t = Token('', 1, 1, 1, 'ID', 'foo', nil)
			assert t.toString()=='"foo" (ID)'
		code
			sb = StringBuilder()
			for c in _text
				# TODO handle \t \c \r
				sb.append(c)
			s = sb.toString()
			if s
				if s.toLower()==_which.toLower()  # if keyword...
					return '"[s]"'
				else
					return '"[s]" ([_which])'
			else
				return '"[_which]"'  # INDENT, DEDENT, etc.


class TokenizerError
	inherits SystemException
	"""
	Raised by nextToken() when there are errors in the source trying to be tokenized.
	"""

	var _tokenizer as Tokenizer

	def construct(tokenizer as Tokenizer, msg as String)
#		base.construct(msg)  @@@@@ TODO
		_tokenizer = tokenizer


class Tokenizer
	"""
	Subclasses often:
		- Override:
			- orderedTokenSpecs
			- unorderedTokenSpecs
			- keywords
	"""

	var _verbosity as int
	var _willAlwaysEndWithNewLine = true
	var _didReset = false

	var _fileName as String
	var _stream as TextReader?

	var _lastToken as IToken?
	var _tokenDefsStack = Stack<of TokenDefSet>()
	var _tokenQueue as Queue<of IToken>  # needed when token methods return lists of tokens

	var _keywordToWhichToken as Dictionary<of String, String>

	var _tokenDefs as List<of TokenDef>
	var _curTokenDef as TokenDef?
	var _lastTokenDef as TokenDef?

	# Source line and location
	var _sourceLine as String?
	var _lastSourceLine as String?
	var _sourceLineIndex as int
	var _lineNum as int
	var _colNum as int
	var _charNum as int

	def construct
		_didReset = false
		_reset()


	## Subclasses often override

	get orderedTokenSpecs as List<of String>
		return List<of String>()

	get unorderedTokenSpecs as List<of String>
		return List<of String>()

	get keywords as String
		"""
		Returns a string containing all keywords separated by spaces.
		Comments starting with a # can be put on individual lines.
		Subclasses often override this to specify keywords.
		"""
		return ''


	## Common properties

	get curTokenDef from var


	## Other

	def _reset
#		.file = nil
#		.readLine = nil
#		.sourceLine = nil
#		.lastToken = nil

		_lineNum = 0
		_colNum  = 0
		_charNum = 0

		_tokenQueue = Queue<of IToken>()
		# _tokenMethods = {}
		_keywordToWhichToken = Dictionary<of String, String>()

		_didReset = true

	def startFileNamed(fileName as String) as Tokenizer  # TODO: change to this
		_fileName = fileName
		_stream = File.openText(fileName)
		.afterStart()
		return this

	def startSource(source as String) as Tokenizer
		_fileName = 'noname'
		_stream = StringReader(source)
		.afterStart()
		return this

	get nextToken as IToken?
		"""
		Consumes a token and returns it, making it the .lastToken.
		Returns nil when there are no tokens left.
		"""
		#assert .readLine, 'Not started.'
		_lastToken = _nextToken
		if _verbosity>=1
			print '<> nextToken() returning [_lastToken]'
		return _lastToken

	def allTokens as List<of IToken>
		"""
		Returns all remaining tokens as a list.
		"""
		tokens = List<of IToken>()
		while true
			t = .nextToken
			if t is nil
				break
			tokens.add(t)
		return tokens

	def restart
		"""
		After calling this, you can use the tokenizer anew.
		Normally you would call this after completing the tokenization of a source file.
		"""
		if _stream
			_stream.close()
			_stream = nil
		_reset()

	def keywordOrWhich(tok as IToken)
		.keywordOrWhich(tok, 'ID')

	def keywordOrWhich(tok as IToken, which as String) as IToken
		"""
		Changes the token to a keyword if it is one, otherwise sets its which to identifier.
		Returns the token.
		This self utility method is typically called by a subclass from the method for the "identifier" token.
		"""
		if _keywordToWhichToken.containsKey(tok.text)
			tok.which = _keywordToWhichToken[which]
		# CC: should be
		# tok.which = _keywordToWhichToken.getOrRet(tok.text, which)
		return tok

	def addTokenSpec(spec as String)
		_tokenDefs.add(.tokenDefForSpec(spec))

	def tokenDefForSpec(spec as String) as TokenDef
		"""
		Returns a TokenDef object for a spec such as
			r'ID			[A-Za-z_][A-Za-z0-9_]*'
		"""
		spec = spec.trim().replace('\t', ' ')

		# CC: should be something like:
		# which, re = spec.split(array(c' '), 2)
		# instead of:
		partNum = 0
		for part as String in spec.split([c' '].toArray(), 2)
			if partNum==0
				which = part
			else
				re = part
			partNum += 1

		re = re.trim()
		assert which
		assert re
		return TokenDef(which, re)

	def afterStart
		"""
		Sets up self attrs
			readLine as callable
			tokenDefs as list
		"""
		assert _didReset, 'Never reset. Probably the subclass overrides _reset() but forgets to invoke base.'
		# create a single list of all token defs in the correct order
		_tokenDefs = List<of TokenDef>()
		if .orderedTokenSpecs
			for spec in .orderedTokenSpecs
				.addTokenSpec(spec)
		others = List<of TokenDef>()
		if .unorderedTokenSpecs
			for spec in .unorderedTokenSpecs
				others.add(.tokenDefForSpec(spec))
		if .keywords
			t = List<of String>()
			for line as String in .keywords.split(c'\n')
				line = line.trim()
				if not line or line.startsWith('#')
					continue
				t.addRange(line.split(nil))
			for word in t
				_keywordToWhichToken[word] = word.toUpper()
		# longest tokens need to be matched first
		# TODO: @@@@@@@@@ others.sort(lambda a, b cmp(a.length, b.length))
		didSort = false
		post while didSort
			didSort = false
			for i = 0 .. others.count-1
				a = others[i]
				b = others[i+1]
				if a.length<b.length
					swap = others[i]
					others[i] = others[i+1]
					others[i+1] = swap
					didSort = true
		_tokenDefs.addRange(others)
		.pushTokenDefs(_tokenDefs)

	# TODO: count should be a CobraCore util method
	# TODO: count should be an extension method of String
	def _countChars(s as String, c as char) as int
		test
			t = Tokenizer()
			assert t._countChars('', c'x')==0
			assert t._countChars('x', c'x')==1
			assert t._countChars('X', c'x')==0  # case sensitive
			assert t._countChars(' ! ! ', c'!')==2
		code
			count = 0
			for ch in s
				if c==ch
					count += 1
			return count

	def _obtainSource as bool
		_sourceLine = _stream.readLine()
		if _sourceLine is nil
			# end of source
			return false
		numLines = _countChars(_sourceLine to String, c'\n')  # CC: to !
		if numLines==0
			if _willAlwaysEndWithNewLine
				_sourceLine += '\n'
		#print '<> sourceLine = %r' % .sourceLine
		if numLines
			if numLines==1
				assert _sourceLine.endsWith('\n'), _sourceLine
			else
				assert false, 'Expecting readline() to return one line instead of many.'
		_sourceLineIndex = 0
		_lineNum += 1
		_colNum = 1
		return true

	get _nextToken as IToken?
		if _tokenQueue
			# eat up queue first
			return _tokenQueue.dequeue()
		if not _sourceLine or _sourceLineIndex==_sourceLine.length
			if not _obtainSource()
				return nil
		try
			for tokenDef in _tokenDefs
				if tokenDef.ignoreCount
					tokenDef.ignoreCount -= 1
					continue
				if not tokenDef.isActive
					continue
				# TODO: get calls working
#				if tokenDef.isActiveCall
#					if not tokenDef.isActiveCall
#						continue
				_curTokenDef = tokenDef  # this enables onTOKENWHICH() methods to access the current tokenDef
				#print '<> Trying to match %r' % tokenDef
				sourceLine = _sourceLine to String  # CC: to !
				match = tokenDef.match(sourceLine, _sourceLineIndex)
				if match is nil or not match.getType().getProperty('success').GetValue(match, nil)  # TODO: if match is nil or not match.`success`
					#print '<> No match on %r for %r' % (tokenDef, .sourceLine[.sourceLineIndex])
					continue
				text = match.toString()
				#print '<> Match! [text] - [tokenDef]'
				# tok = Token(_fileName, _lineNum, _colNum, _charNum, tokenDef.which, text, text) to ? # CC
				tok as IToken?
				tok = Token(_fileName, _lineNum, _colNum, _charNum, tokenDef.which, text, text)
				len = text.length
				_sourceLineIndex += len
				_colNum += len
				_charNum += len

				# enable methods to customize handling of tokens
				reinvoke = false
				# TODO: get takenMethods working
#				tokenMethod = _tokenMethods.get(tok.which, NotFound)
#				if tokenMethod is NotFound
#					tokenMethod = _tokenMethods[tok.which] = getattr('on'+tok.which, nil)
#				if tokenMethod
#					stuff = tokenMethod(tok)
#					if stuff is nil
#						# skip token, so go to the next one
#						reinvoke = true
#					elif inherits(stuff, Token)
#						tok = stuff
#					elif inherits(stuff, list)
#						# handle inserting tokens into the stream of tokens
#						_tokenQueue.extend(stuff)
#						reinvoke = true

				# finished with current line?
				if _sourceLineIndex==_sourceLine.length
					_lastSourceLine = _sourceLine
					_sourceLine = nil
					_sourceLineIndex = -1

				# handle token skipping
				if reinvoke
					tok = _nextToken

				# yay!
				return tok
		finally
			_curTokenDef = nil

		# no match
		_error('Lexical error at [_sourceLine[_sourceLineIndex]]')
		return nil

	def pushTokenDefs(defs as List<of TokenDef>)
		pass

	def _error(msg as String)
		throw TokenizerError(this, msg)


class TokenDef

	var _which as String
	var _regExSource as String
	var _re as Regex
	var _ignoreCount as int
	var _isActive = true
	var _isActiveCall as Object # @@@@??

	def construct(which as String, regExSource as String)
		_which = which
		_regExSource = regExSource
		_re = Regex(_regExSource)

	pro which from var

	get regExSource from var

	get length as int
		return _regExSource.length

	get re from var

	pro ignoreCount from var

	pro isActive from var

	def match(input as String, startAt as int) as Match
		return _re.match(input, startAt)

	def toString as String is override
		return '[.getType().name]([_which], [_regExSource], [_re])'


class TokenDefSet

	var _defs as List<of TokenDef>
	var _defsByWhich as Dictionary<of String, TokenDef>

	def construct(defs as List<of TokenDef>, defsByWhich as Dictionary<of String, TokenDef>)
		_defs = defs
		_defsByWhich = defsByWhich

	get defs from var

	get defsByWhich from var


namespace System

	namespace IO

		class StringReader
			is fake
			inherits TextReader
			pass

		class Stream
			is fake
			pass

		class MemoryStream
			is fake
			pass

	namespace Text

		namespace RegularExpressions

			class Regex
				is fake
				pass

			class Match
				is fake
				pass
