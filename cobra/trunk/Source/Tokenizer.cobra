"""
Tokenizer.py


Requirements of a nice tokenizer

	- Produce tokens on demand instead of all at once.

	- Consume text in lines so that text can be streamed in.

	- Accurately and automatically report for each token
		which kind, text, line number, column number, character index and length

	- Allow methods to intercept when a token is encountered in order to
		- modify it
		- skip it
		- replace it by a list of tokens

	- Unit tests built-in

TODO:

	- Reorder in top down fashion

	- Go back and add test, require and ensure whereever possible

	- Check for TODO

"""

use System.Text.RegularExpressions

interface IToken

	pro which as String
	pro text as String
	get length as int

	get fileName as String
	get lineNum as int
	get colNum as int
	get charNum as int

	get copy as IToken

	pro nextToken as IToken
		"""
		Use this to "insert" extra tokens into the token stream from an onWHICH() method.
		"""


class Token
	implements IToken

	var _fileName as String
	var _lineNum as int
	var _colNum as int
	var _charNum as int
	var _which as String
	var _text as String
	var _value as Object?  # TODO I think it would make more sense for this to be dynamic
	var _nextToken as IToken

	def construct(fileName as String, lineNum as int, colNum as int, charNum as int, which as String, text as String, value as Object?)
		require
			fileName
			fileName is not nil
			fileName.length > 0  # CC: fileName
			lineNum > 0
			colNum > 0
			charNum > 0
		code
			_fileName = fileName
			_lineNum = lineNum
			_colNum = colNum
			_charNum = charNum
			_which = which
			_text = text
			_value = value

	pro which from var

	pro text from var

	get length as int
		return _text.length

	get fileName from var

	get lineNum from var

	get colNum from var

	get charNum from var

	pro nextToken from var

	get copy as IToken
		# TODO: should this:
		t = Token(_fileName, _lineNum, _colNum, _charNum, _which, _text, _value)
		# be this:
		# t = .getType()(_fileName, _lineNum, _colNum, _charNum, _which, _text, _value)
		# and if so, how does performance change?
		if _nextToken
			t.nextToken = _nextToken.copy
		return t

	def toString as String is override
		test
			t = Token('(noname)', 1, 1, 1, 'ID', 'foo', nil)
			assert t.toString()=='"foo" (ID)'
		code
			sb = StringBuilder()
			for c in _text
				# TODO handle \t \c \r
				sb.append(c)
			s = sb.toString()
			if s
				if _which is nil or s.toLower()==_which.toLower()  # if keyword...
					return '"[s]"'
				else
					return '"[s]" ([_which])'
			else
				return '"[_which]"'  # INDENT, DEDENT, etc.


class TokenizerError
	inherits SystemException
	"""
	Raised by nextToken() when there are errors in the source trying to be tokenized.
	"""

	var _tokenizer as Tokenizer
	var _msg as String

	def construct(tokenizer as Tokenizer, msg as String)
		# base.construct(msg) CC: re-enable this after fixing and axe next line
		_msg = msg
		_tokenizer = tokenizer

	get message as String is override
		return _msg


class Tokenizer
	"""
	Subclasses often:
		- Override:
			- orderedTokenSpecs
			- unorderedTokenSpecs
			- keywords
	"""

	var _verbosity as int
	var _willAlwaysEndWithNewLine = true
	var _didReset = false

	var _fileName as String?
	var _stream as TextReader?

	var _lastToken as IToken?
	var _tokenDefsStack = Stack<of TokenDefSet>()
	var _tokenQueue as Queue<of IToken>  # needed when token methods return lists of tokens

	var _keywordToWhichToken as Dictionary<of String, String>

	var _tokenDefs as List<of TokenDef>
	var _tokenDefsByWhich as Dictionary<of String, TokenDef>
	var _curTokenDef as TokenDef?
	var _lastTokenDef as TokenDef?

	# Source line and location
	var _sourceLine as String?
	var _lastSourceLine as String?
	var _sourceLineIndex as int
	var _lineNum as int
	var _colNum as int
	var _charNum as int

	def construct
		_didReset = false
		_reset()


	## Subclasses often override

	get orderedTokenSpecs as List<of String>
		return List<of String>()

	get unorderedTokenSpecs as List<of String>
		return List<of String>()

	get keywords as String
		"""
		Returns a string containing all keywords separated by spaces.
		Comments starting with a # can be put on individual lines.
		Subclasses often override this to specify keywords.
		"""
		return ''


	## Common properties

	get curTokenDef from var


	## Other

	def _reset
		_fileName = nil
		_stream = nil
		_lastToken = nil

		_sourceLine = nil
		_lastSourceLine = nil
		_lineNum = 0
		_colNum  = 1
		_charNum = 1

		_tokenQueue = Queue<of IToken>()
		# _tokenMethods = {}
		_keywordToWhichToken = Dictionary<of String, String>()

		_didReset = true

	def startFileNamed(fileName as String) as Tokenizer  # TODO: change to this
		_fileName = fileName
		_stream = File.openText(fileName)
		.afterStart()
		return this

	def startSource(source as String) as Tokenizer
		_fileName = 'noname'
		_stream = StringReader(source)
		.afterStart()
		return this

	get nextToken as IToken?
		"""
		Consumes a token and returns it, making it the .lastToken.
		Returns nil when there are no tokens left.
		"""
		#assert .readLine, 'Not started.'
		_lastToken = _nextToken
		if _verbosity>=1
			print '<> nextToken() returning [_lastToken]'
		return _lastToken

	def allTokens as List<of IToken>
		"""
		Returns all remaining tokens as a list.
		"""
		tokens = List<of IToken>()
		while true
			t = .nextToken
			if t is nil
				break
			tokens.add(t)
		return tokens

	def restart
		"""
		After calling this, you can use the tokenizer anew.
		Normally you would call this after completing the tokenization of a source file.
		"""
		if _stream
			_stream.close()
			_stream = nil
		_reset()

	def keywordOrWhich(tok as IToken)
		.keywordOrWhich(tok, 'ID')

	def keywordOrWhich(tok as IToken, which as String) as IToken
		"""
		Changes the token to a keyword if it is one, otherwise sets its which.
		Returns the token.
		This self utility method is typically called by a subclass from the method for the
		"identifier" token.
		"""
		if _keywordToWhichToken.containsKey(tok.text)
			tok.which = _keywordToWhichToken[which]
		# CC: should be
		# tok.which = _keywordToWhichToken.getOrRet(tok.text, which)
		return tok

	def addTokenSpec(spec as String)
		_tokenDefs.add(.tokenDefForSpec(spec))

	def tokenDefForSpec(spec as String) as TokenDef
		"""
		Returns a TokenDef object for a spec such as
			r'ID			[A-Za-z_][A-Za-z0-9_]*'
		"""
		spec = spec.trim().replace('\t', ' ')

		# CC: should be something like:
		# which, re = spec.split(array(c' '), 2)
		# instead of:
		partNum = 0
		for part as String in spec.split([c' '].toArray(), 2)
			if partNum==0
				which = part
			else
				re = part
			partNum += 1
		assert partNum==2, 'Got [partNum] part(s) for spec "[spec]" instead of 2'

		re = re.trim()
		assert which
		assert re
		return TokenDef(which, re)

	def afterStart
		"""
		Sets up self attrs
			readLine as callable
			tokenDefs as list
		"""
		assert _didReset, 'Never reset. Probably the subclass overrides _reset() but forgets to invoke base.'
		# create a single list of all token defs in the correct order
		_tokenDefs = List<of TokenDef>()
		if .orderedTokenSpecs
			for spec in .orderedTokenSpecs
				.addTokenSpec(spec)
		others = List<of TokenDef>()
		if .unorderedTokenSpecs
			for spec in .unorderedTokenSpecs
				others.add(.tokenDefForSpec(spec))
		if .keywords
			t = List<of String>()
			for line as String in .keywords.split(c'\n')
				line = line.trim()
				if not line or line.startsWith('#')
					continue
				t.addRange(line.split(nil))
			for word in t
				_keywordToWhichToken[word] = word.toUpper()
		# longest tokens need to be matched first
		# CC: others.sort(lambda a, b cmp(a.length, b.length))
		didSort = false
		post while didSort
			didSort = false
			for i = 0 .. others.count-1
				a = others[i]
				b = others[i+1]
				if a.length<b.length
					swap = others[i]
					others[i] = others[i+1]
					others[i+1] = swap
					didSort = true
		_tokenDefs.addRange(others)
		.pushTokenDefs(_tokenDefs)

	# TODO: count should be a CobraCore util method
	# TODO: count should be an extension method of String
	def _countChars(s as String, c as char) as int
		test
			t = Tokenizer()
			assert t._countChars('', c'x')==0
			assert t._countChars('x', c'x')==1
			assert t._countChars('X', c'x')==0  # case sensitive
			assert t._countChars(' ! ! ', c'!')==2
		code
			count = 0
			for ch in s
				if c==ch
					count += 1
			return count

	def _obtainSource as bool
		_sourceLine = _stream.readLine()
		if _sourceLine is nil
			# end of source
			return false
		numLines = _countChars(_sourceLine to String, c'\n')  # CC: to !
		if numLines==0
			if _willAlwaysEndWithNewLine
				_sourceLine += '\n'
		#print '<> sourceLine = %r' % .sourceLine
		if numLines
			if numLines==1
				assert _sourceLine.endsWith('\n'), _sourceLine
			else
				assert false, 'Expecting readline() to return one line instead of many.'
		_sourceLineIndex = 0
		_lineNum += 1
		_colNum = 1
		return true

	get _nextToken as IToken?
		if _tokenQueue
			# eat up queue first
			return _tokenQueue.dequeue()
		if not _sourceLine or _sourceLineIndex==_sourceLine.length
			if not _obtainSource()
				return nil
		try
			for tokenDef in _tokenDefs
				if tokenDef.ignoreCount
					tokenDef.ignoreCount -= 1
					continue
				if not tokenDef.isActive
					continue
				# TODO: get calls working
#				if tokenDef.isActiveCall
#					if not tokenDef.isActiveCall
#						continue
				_curTokenDef = tokenDef  # this enables onTOKENWHICH() methods to access the current tokenDef
				#print '<> Trying to match %r' % tokenDef
				sourceLine = _sourceLine to String  # CC: to !
				match = tokenDef.match(sourceLine, _sourceLineIndex)
				if match is nil or not match.getType().getProperty('Success').getValue(match, nil)  # TODO: if match is nil or not match.`success`
					#print '<> No match on [tokenDef] for [_sourceLine[_sourceLineIndex]]'
					continue
				if match.index>_sourceLineIndex
					# TODO: there's no option I can see to get the regex to fail to match if it can't
					# match the first character! wtf? This will kill performance and probably dictate
					# that Cobra go to a non-regex solution, if it wasn't already
					continue
				text = match.toString()
				#print '<> Match! [text] - [tokenDef]'
				# tok = Token(_fileName, _lineNum, _colNum, _charNum, tokenDef.which, text, text) to ? # CC
				tok as IToken?
				tok = Token(_fileName, _lineNum, _colNum, _charNum, tokenDef.which, text, text)
				len = text.length
				_sourceLineIndex += len
				_colNum += len
				_charNum += len

				# enable methods to customize handling of tokens
				reinvoke = false

				# TODO: test if caching the methods is faster
				methName = 'On' + tok.which
				meth = .getType().getMethod(methName) # , [IToken].toArray())
				if meth is not nil
					tok = meth.invoke(this, [tok].toArray()) to IToken
					if tok is nil
						# skip token, so go to the next one
						reinvoke = true
					else
						# TODO: could probably make this more efficient by axing the queue and just checking for nextToken in this method
						while tok
							_tokenQueue.enqueue(tok)
							tok = tok.nextToken
						reinvoke = true  # to pick up the queue

				# finished with current line?
				if _sourceLineIndex==_sourceLine.length
					_lastSourceLine = _sourceLine
					_sourceLine = nil
					_sourceLineIndex = -1

				# handle token skipping
				if reinvoke
					tok = _nextToken

				# yay!
				return tok
		finally
			_curTokenDef = nil

		# no match
		_error('Lexical error at [_lineNum]([_sourceLineIndex]) - [_sourceLine[_sourceLineIndex]]')
		return nil

	def pushTokenDefs(defs as List<of TokenDef>)
		defsByWhich = Dictionary<of String, TokenDef>()
		for tokenDef in defs
			assert not defsByWhich.containsKey(tokenDef.which), tokenDef
			defsByWhich[tokenDef.which] = tokenDef
		_tokenDefsStack.push(TokenDefSet(defs, defsByWhich))
		_tokenDefs = defs
		_tokenDefsByWhich = defsByWhich

	def popTokenDefs
		_tokenDefsStack.pop()
		if _tokenDefsStack
			tokenDefSet as TokenDefSet = _tokenDefsStack.peek() to TokenDefSet # CC: shouldn't need the 'as' or the 'to'
			defs = tokenDefSet.defs # CC: add: to ?
			defsByWhich = tokenDefSet.defsByWhich # CC: add: to ?
		else
			defs = nil to passthrough  # CC: axe cast after fixing CCs above
			defsByWhich = nil to passthrough  # CC: axe cast after fixing CCs above
		_tokenDefs = defs
		_tokenDefsByWhich = defsByWhich

	def _error(msg as String)
		throw TokenizerError(this, msg)


class TokenDef

	var _which as String
	var _regExSource as String
	var _re as Regex
	var _ignoreCount as int
	var _isActive = true
	var _isActiveCall as Object # @@@@??

	def construct(which as String, regExSource as String)
		_which = which
		_regExSource = regExSource
		_re = Regex(_regExSource)

	pro which from var

	get regExSource from var

	get length as int
		return _regExSource.length

	get re from var

	pro ignoreCount from var

	pro isActive from var

	def match(input as String, startAt as int) as Match
		return _re.match(input, startAt)

	def toString as String is override
		return '[.getType().name]([_which], [_regExSource], [_re])'


class TokenDefSet

	var _defs as List<of TokenDef>
	var _defsByWhich as Dictionary<of String, TokenDef>

	def construct(defs as List<of TokenDef>, defsByWhich as Dictionary<of String, TokenDef>)
		_defs = defs
		_defsByWhich = defsByWhich

	get defs from var

	get defsByWhich from var


class TestTokenizer
	inherits Tokenizer

	var _idCount as int

	get idCount from var

	get orderedTokenSpecs as List<of String> is override
		return [
			'OPEN_IF		ifx\\(',
			ns'ID			[A-Za-z_][A-Za-z0-9_]*',
			ns'SPACE		[ ]+',
			'NEWLINE		\\n',
			]

	get unorderedTokenSpecs as List<of String> is override
		return [
			'DOT			\\.',
			'COLON			:',
			'PLUS			\\+',
			'ASSIGN		=',
			'EQUALS		==',
			]

	get keywords as String is override
		return 'assert if else'

	def _reset is override
		base._reset()
		_idCount = 0

	def onID(tok as IToken) as IToken?
		assert tok
		_idCount += 1
		return tok

	def onSPACE(tok as IToken) as IToken?
		return nil

	def onNEWLINE(tok as IToken) as IToken?
		return tok

	def onDOT(tok as IToken) as IToken?
		tok.nextToken = tok.copy
		return tok

	test
		# basics
		tt = TestTokenizer()
		tt.startSource('hello there')
		tokens = tt.allTokens()
		.checkTokens(tokens, 'ID ID NEWLINE')
		assert tt.idCount==2

		# the tokenizer lets methods insert tokens
		tt.restart()
		tt.startSource('hello.there')
		tokens = tt.allTokens()
		.checkTokens(tokens, 'ID DOT DOT ID NEWLINE')

		# tokens know their line numbers and columns and lengths
		tt.restart()
		tt.startSource('hello\nthere\n you\n ')
		tokens = tt.allTokens()
		# CC: tokens = for tok in tokens select tok if tok.which<>'NEWLINE' -- or something like that
		for i = 0 .. tokens.count
			if tokens[i].which=='NEWLINE'
				tokens.removeAt(i)
				i -= 1
		assert tokens.count==3, tokens.count
		hello = tokens[0]  # CC:  hello, there, you = tokens
		there = tokens[1]
		you = tokens[2]

		assert hello.lineNum==1
		assert hello.colNum==1
		assert hello.length==5
		assert there.lineNum==2
		assert there.colNum==1
		assert there.length==5
		assert you.lineNum==3
		assert you.colNum==2
		assert you.length==3

	def checkTokens(tokens as List<of IToken>, s as String)
		is shared
		sb = StringBuilder()
		sep = ''
		for t in tokens
			sb.append(sep)
			sb.append(t.which)
			sep = ' '
		tokensStr = sb.toString()
		assert tokensStr==s, 'tokensStr=[tokensStr], s=[s]'



namespace System

	namespace IO

		class StringReader
			is fake
			inherits TextReader
			pass

		class Stream
			is fake
			pass

		class MemoryStream
			is fake
			pass

	namespace Text

		namespace RegularExpressions

			class Regex
				is fake
				pass

			class Match
				is fake
				get index as int
					return 0
