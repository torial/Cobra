"""
Tokenizer.py


Requirements of a nice tokenizer

	- Produce tokens on demand instead of all at once.

	- Consume text in lines so that text can be streamed in.

	- Accurately and automatically report for each token
		which kind, text, line number, column number, character index and length

	- Allow methods to intercept when a token is encountered in order to
		- modify it
		- skip it
		- replace it by a list of tokens

	- Unit tests built-in

TODO:

	- Reorder in top down fashion

	- Check for TODO

"""

use System.Text.RegularExpressions
use System.Reflection


interface IToken

	pro which as String
	pro text as String
	get length as int
	pro value as Object?
		"""
		Returns the value the token represents. This is normally the same as the text unless there
		is specific reason to give another value, such as the real integer value that an integer
		token represents.
		"""

	get fileName as String
	get lineNum as int
	get colNum as int
	get charNum as int

	get copy as IToken

	pro nextToken as IToken
		"""
		Use this to "insert" extra tokens into the token stream from an onWHICH() method.
		"""

	def toTechString as String


class Token
	implements IToken

	shared
		var _empty as Token

		get empty as Token  # CC: as same
			if _empty is nil
				_empty = Token('(empty)', 1, 1, 1, '(EMPTY)', '', nil)
			return _empty

	var _fileName as String
	var _lineNum as int
	var _colNum as int
	var _charNum as int
	var _which as String
	var _text as String
	var _value as Object?  # TODO I think it would make more sense for this to be dynamic
	var _nextToken as IToken?

	def construct(fileName as String, lineNum as int, colNum as int, charNum as int, which as String, text as String, value as Object?)
		require
			fileName
			lineNum > 0
			colNum > 0
			charNum > 0
			which
		code
			_fileName = fileName
			_lineNum = lineNum
			_colNum = colNum
			_charNum = charNum
			_which = which
			_text = text
			_value = value

	pro which from var

	pro text from var

	get length as int
		return _text.length

	pro value from var

	get fileName from var

	get lineNum from var
		# CC: ensure result > 1

	get colNum from var
		# CC: ensure result > 1

	get charNum from var
		# CC: ensure result > 1

	pro nextToken from var

	get copy as IToken
		ensure
			result.which == .which
			result.text == .text
			result.value == .value
			.nextToken implies result.nextToken
		test
			t = Token('(noname)', 1, 1, 1, 'ID', 'foo', nil)
			u = t.copy
			assert t is not u
			assert t.which == 'ID'
		code
			# TODO: should this:
			t = Token(_fileName, _lineNum, _colNum, _charNum, _which, _text, _value)
			# be this:
			# t = .getType()(_fileName, _lineNum, _colNum, _charNum, _which, _text, _value)
			# and if so, how does performance change?
			if _nextToken
				t.nextToken = _nextToken.copy
			return t

	def toString as String is override
		test
			t = Token('(noname)', 1, 1, 1, 'ID', 'foo', nil)
			assert t.toString()=='"foo" (ID)'
		code
			sb = StringBuilder()
			for c in _text
				branch c
					on c'\t': sb.append('\\t')
					on c'\r': sb.append('\\r')
					on c'\n': sb.append('\\n')
					else: sb.append(c)
			s = sb.toString()
			if s
				if _which is nil or s.toLower()==_which.toLower()  # if keyword...
					return '"[s]"'
				else
					return '"[s]" ([_which])'
			else
				return '"[_which]"'  # INDENT, DEDENT, etc.

	def toTechString as String
		return '[.getType().name]([.which], [CobraCore.toTechString(.text)], [CobraCore.toTechString(.value)], ln [.lineNum], col [.colNum], [.fileName])'


class TokenFix
	inherits Token
	"""
	This hack is due to certain C# circumstances where referring to "Token.empty" in Cobra
	does not translate well to C# which wants to interpret "Token" as a property instead
	of the class.
	"""

	def construct(fileName as String, lineNum as int, colNum as int, charNum as int, which as String, text as String, value as Object?)
		base.construct(fileName, lineNum, colNum, charNum, which, text, value)


class TokenizerError
	inherits SystemException
	"""
	Raised by nextToken() when there are errors in the source trying to be tokenized.
	"""

	var _tokenizer as Tokenizer
	var _msg as String

	def construct(tokenizer as Tokenizer, msg as String)
		require msg
		# base.construct(msg) CC: re-enable this after fixing and axe next line
		_msg = msg
		_tokenizer = tokenizer

	get message as String is override
		return _msg

	get tokenizer from var


class Tokenizer
	"""
	Subclasses often:
		- Override:
			- orderedTokenSpecs
			- unorderedTokenSpecs
			- keywords
	"""

	var _verbosity as int
	var _willAlwaysEndWithNewLine = true
	var _didReset = false

	var _fileName as String?
	var _stream as TextReader?

	var _lastToken as IToken?
	var _tokenDefsStack = Stack<of TokenDefSet>()
	var _tokenQueue as Queue<of IToken>  # needed when token methods return lists of tokens

	var _keywordToWhichToken as Dictionary<of String, String>

	var _tokenDefs as List<of TokenDef>
	var _tokenDefsByWhich as Dictionary<of String, TokenDef>
	var _curTokenDef as TokenDef?
	var _lastTokenDef as TokenDef?

	# Source line and location
	var _sourceLine as String?
	var _lastSourceLine as String?
	var _sourceLineIndex as int
	var _lineNum as int
	var _colNum as int
	var _charNum as int

	def construct
		_didReset = false
		_reset()


	## Subclasses often override

	get orderedTokenSpecs as List<of String>
		return List<of String>()

	get unorderedTokenSpecs as List<of String>
		return List<of String>()

	get keywords as String
		"""
		Returns a string containing all keywords separated by spaces.
		Comments starting with a # can be put on individual lines.
		Subclasses often override this to specify keywords.
		"""
		return ''


	## Common properties

	get curTokenDef from var

	get lastToken from var


	## Other

	def _reset
		_fileName = nil
		_stream = nil
		_lastToken = nil

		_sourceLine = nil
		_lastSourceLine = nil
		_lineNum = 0
		_colNum  = 1
		_charNum = 1

		_tokenQueue = Queue<of IToken>()
		# _tokenMethods = {}
		_keywordToWhichToken = Dictionary<of String, String>()

		_didReset = true

	def startFileNamed(fileName as String) as Tokenizer  # TODO: change to this
		_fileName = fileName
		_stream = File.openText(fileName)
		.afterStart()
		return this

	def startSource(source as String) as Tokenizer
		return .startSource('(no file name)', source)

	def startSource(fileName as String, source as String) as Tokenizer
		if false
			print '**********************************************************************'
			print source
			print '**********************************************************************'
		_fileName = fileName
		_stream = StringReader(source)
		.afterStart()
		return this

	get nextToken as IToken?
		"""
		Consumes a token and returns it, making it the .lastToken.
		Returns nil when there are no tokens left.
		"""
# TODO: re-enable this		ensure .lastToken == result
		#assert .readLine, 'Not started.'
		_lastToken = _nextToken
		if _verbosity>=1
			print '<> nextToken() returning [_lastToken]'
		return _lastToken

	def allTokens as List<of IToken>
		"""
		Returns all remaining tokens as a list.
		"""
		tokens = List<of IToken>()
		while true
			t = .nextToken
			if t is nil
				break
			tokens.add(t)
		return tokens

	def restart
		"""
		After calling this, you can use the tokenizer anew.
		Normally you would call this after completing the tokenization of a source file.
		"""
		if _stream
			_stream.close()
			_stream = nil
		_reset()

	def keywordOrWhich(tok as IToken) as IToken
		return .keywordOrWhich(tok, 'ID')

	def keywordOrWhich(tok as IToken, which as String) as IToken
		"""
		Changes the token to a keyword if it is one, otherwise sets its which.
		Returns the token.
		This self utility method is typically called by a subclass from the method for the
		"identifier" token.
		"""
		ensure result is tok
		if _keywordToWhichToken.containsKey(tok.text)
			tok.which = _keywordToWhichToken[tok.text]
		# CC: should be
		# tok.which = _keywordToWhichToken.getOrRet(tok.text, which)
		return tok

	def addTokenSpec(spec as String)
		_tokenDefs.add(.tokenDefForSpec(spec))

	def tokenDefForSpec(spec as String) as TokenDef
		"""
		Returns a TokenDef object for a spec such as
			r'ID			[A-Za-z_][A-Za-z0-9_]*'
		"""
		spec = spec.trim().replace('\t', ' ')

		# CC: should be something like:
		# which, re = spec.split(array(c' '), 2)
		# instead of:
		partNum = 0
		for part as String in spec.split([c' '].toArray(), 2)
			if partNum==0
				which = part
			else
				re = part
			partNum += 1
		assert partNum==2, 'Got [partNum] part(s) for spec "[spec]" instead of 2'

		re = re.trim()
		assert which
		assert re
		return TokenDef(which, re)

	def afterStart
		"""
		Sets up self attrs
			readLine as callable
			tokenDefs as list
		"""
		ensure
			_tokenDefs is not nil
		code
			assert _didReset, 'Have not reset. Probably the subclass overrides _reset() but forgets to invoke base.'
			# create a single list of all token defs in the correct order
			_tokenDefs = List<of TokenDef>()
			if .orderedTokenSpecs
				for spec in .orderedTokenSpecs
					.addTokenSpec(spec)
			others = List<of TokenDef>()
			if .unorderedTokenSpecs
				for spec in .unorderedTokenSpecs
					others.add(.tokenDefForSpec(spec))
			if .keywords
				t = List<of String>()
				for line as String in .keywords.split(c'\n')
					line = line.trim()
					if not line or line.startsWith('#')
						continue
					t.addRange(line.split(nil))
				for word in t
					_keywordToWhichToken[word] = word.toUpper()
			# longest tokens need to be matched first
			# CC: others.sort(lambda a, b cmp(a.length, b.length))
			didSort = false
			post while didSort
				didSort = false
				for i = 0 .. others.count-1
					a = others[i]
					b = others[i+1]
					if a.length<b.length
						swap = others[i]
						others[i] = others[i+1]
						others[i+1] = swap
						didSort = true
			_tokenDefs.addRange(others)
			.pushTokenDefs(_tokenDefs)

	# TODO: count should be a CobraCore util method
	# TODO: count should be an extension method of String
	def _countChars(s as String, c as char) as int
		test
			t = Tokenizer()
			assert t._countChars('', c'x')==0
			assert t._countChars('x', c'x')==1
			assert t._countChars('X', c'x')==0  # case sensitive
			assert t._countChars(' ! ! ', c'!')==2
		code
			count = 0
			for ch in s
				if c==ch
					count += 1
			return count

	def _obtainSource as bool
		ensure
			result implies _sourceLine is not nil
			result implies _lineNum == old _lineNum + 1
		code
			_sourceLine = _stream.readLine()
			if _sourceLine is nil
				# end of source
				return false
			numLines = _countChars(_sourceLine to String, c'\n')  # CC: to !
			if numLines==0
				if _willAlwaysEndWithNewLine
					_sourceLine += '\n'
			#print '<> sourceLine = %r' % .sourceLine
			if numLines
				if numLines==1
					assert _sourceLine.endsWith('\n'), _sourceLine
				else
					assert false, 'Expecting readline() to return one line instead of many.'
			_sourceLineIndex = 0
			_lineNum += 1
			_colNum = 1
			return true

	get _nextToken as IToken?
		"""
		This is the core brain of the tokenizer.
		The primary logic for matching tokens is here.
		"""
		if _tokenQueue
			# eat up queue first
			return _tokenQueue.dequeue()
		if not _sourceLine
			if not _obtainSource()
				return nil
		try
			for tokenDef in _tokenDefs
				if tokenDef.ignoreCount
					tokenDef.ignoreCount -= 1
					continue
				if not tokenDef.isActive
					continue
				if _sourceLineIndex>0 and tokenDef.requiresBOL
					continue
				if not .isActiveCall(tokenDef)
					continue
				#print '<> Trying to match [tokenDef]'
				sourceLine = _sourceLine to String  # CC: to !
				#print '_sourceLineIndex=[_sourceLineIndex]'
				match = tokenDef.match(sourceLine)
				# CC: if match is nil or not match.`success`
				if match is nil or not $sharp('match.Success')
				#if match is nil or not match.getType().getProperty('Success').getValue(match, nil)
					#print '<> No match on [tokenDef] for [_sourceLine[_sourceLineIndex]]'
					continue
				assert match.index==0
				_lastTokenDef = _curTokenDef
				_curTokenDef = tokenDef  # this enables onTOKENWHICH() methods to access the current tokenDef
				text = match.toString()
				#print '<> Match! [CobraCore.toTechString(text)] - [tokenDef]'
				# tok = Token(_fileName, _lineNum, _colNum, _charNum, tokenDef.which, text, text) to ? # CC
				tok as IToken?
				tok = Token(_fileName, _lineNum, _colNum, _charNum, tokenDef.which, text, text)
				len = text.length
				_colNum += len
				_charNum += len
				_sourceLineIndex += len
				_sourceLine = _sourceLine.substring(len)

				# enable methods to customize handling of tokens
				reinvoke = false

				# TODO: test if caching the methods is faster
				methName = 'On' + tok.which
				meth = .getType().getMethod(methName)
				if meth is not nil
					try
						tok = meth.invoke(this, $sharp(r'new IToken[1] {tok}')) to IToken  # CC: to IToken?
						#tok = meth.invoke(this, [tok].toArray()) to IToken  # CC: to IToken?
					catch tie as TargetInvocationException
						throw tie.innerException
					if tok is nil
						# skip token, so go to the next one
						reinvoke = true
					else
						# TODO: could probably make this more efficient by axing the queue and just checking for nextToken in this method
						while tok
							_tokenQueue.enqueue(tok)
							tok = tok.nextToken
						reinvoke = true  # to pick up the queue

				# finished with current line?
				if _sourceLine.length==0
					_lastSourceLine = _sourceLine
					_sourceLine = nil
					_sourceLineIndex = -1

				# handle token skipping
				if reinvoke
					tok = _nextToken

				# yay!
				return tok
		finally
			_curTokenDef = nil

		# no match
		_error('Lexical error at [_lineNum]([_sourceLineIndex]) - [_sourceLine[_sourceLineIndex]]')
		return nil

	def pushTokenDefs(defs as List<of TokenDef>)
		ensure
			_tokenDefs is defs
			_tokenDefsByWhich.count is defs.count
		code
			defsByWhich = Dictionary<of String, TokenDef>()
			for tokenDef in defs
				assert not defsByWhich.containsKey(tokenDef.which), tokenDef
				defsByWhich[tokenDef.which] = tokenDef
			_tokenDefsStack.push(TokenDefSet(defs, defsByWhich))
			_tokenDefs = defs
			_tokenDefsByWhich = defsByWhich

	def popTokenDefs
		require
			_tokenDefsStack.count > 0
		code
			_tokenDefsStack.pop()
			if _tokenDefsStack
				tokenDefSet as TokenDefSet = _tokenDefsStack.peek() to TokenDefSet # CC: shouldn't need the 'as' or the 'to'
				defs = tokenDefSet.defs # CC: add: to ?
				defsByWhich = tokenDefSet.defsByWhich # CC: add: to ?
			else
				defs = nil to passthrough  # CC: axe cast after fixing CCs above
				defsByWhich = nil to passthrough  # CC: axe cast after fixing CCs above
			_tokenDefs = defs
			_tokenDefsByWhich = defsByWhich

	def isActiveCall(tok as TokenDef) as bool
		return true

	def _error(msg as String)
		throw TokenizerError(this, msg)

	def checkTokens(tokens as List<of IToken>, expected as String)
		is shared
		"""
		Returns true if the list of tokens "matches" the string.
		This is a utility method to aid with testing.
		"""
		sb = StringBuilder()
		sep = ''
		for t in tokens
			sb.append(sep)
			sb.append(t.which)
			sep = ' '
		tokensStr = sb.toString()
		assert tokensStr==expected, 'tokensStr=[tokensStr], expected=[expected]'


class TokenDef

	shared
		var _compiledRegExes = Dictionary<of String, Regex>()

	var _which as String
	var _regExSource as String
	var _re as Regex
	var _requiresBOL as bool
	var _ignoreCount as int
	var _isActive = true
	var _isActiveCall as Object # @@@@??

	def construct(which as String, regExSource as String)
		require
			which
			regExSource
		ensure
			.re
		code
			_requiresBOL = regExSource.startsWith('^')
			if not _requiresBOL
				regExSource = '^' + regExSource
			_which = which
			_regExSource = regExSource
			if _compiledRegExes.containsKey(regExSource)
				_re = _compiledRegExes[regExSource]
			else
				_re = Regex(_regExSource, $sharp('RegexOptions.Compiled'))
				_compiledRegExes[regExSource] = _re

	pro which from var

	get regExSource from var

	get length as int
		return _regExSource.length

	get requiresBOL from var

	get re from var

	pro ignoreCount from var

	pro isActive from var

	def match(input as String) as Match
		require
			input
		code
			return _re.match(input)

	def match(input as String, startAt as int) as Match
		require
			input
			startAt >= 0
		code
			return _re.match(input, startAt)

	def toString as String is override
		return '[.getType().name]([_which], [_regExSource], [_re])'


class TokenDefSet

	var _defs as List<of TokenDef>
	var _defsByWhich as Dictionary<of String, TokenDef>

	def construct(defs as List<of TokenDef>, defsByWhich as Dictionary<of String, TokenDef>)
		require
			defs
			defsByWhich
		code
			_defs = defs
			_defsByWhich = defsByWhich

	get defs from var

	get defsByWhich from var


class TestTokenizer
	inherits Tokenizer

	var _idCount as int

	get idCount from var

	get orderedTokenSpecs as List<of String> is override
		return [
			'OPEN_IF		ifx\\(',
			ns'ID			[A-Za-z_][A-Za-z0-9_]*',
			ns'SPACE		[ ]+',
			'NEWLINE		\\n',
			]

	get unorderedTokenSpecs as List<of String> is override
		return [
			'DOT		\\.',
			'COLON		:',
			'PLUS		\\+',
			'ASSIGN		=',
			'EQUALS		==',
			]

	get keywords as String is override
		return 'assert if else'

	def _reset is override
		base._reset()
		_idCount = 0

	def onID(tok as IToken) as IToken?
		_idCount += 1
		return tok

	def onSPACE(tok as IToken) as IToken?
		return nil

	def onNEWLINE(tok as IToken) as IToken?
		return tok

	def onDOT(tok as IToken) as IToken?
		tok.nextToken = tok.copy
		return tok

	test
		# basics
		tt = TestTokenizer()
		tt.startSource('hello there')
		tokens = tt.allTokens()
		.checkTokens(tokens, 'ID ID NEWLINE')
		assert tt.idCount==2

		# the tokenizer lets methods insert tokens
		tt.restart()
		tt.startSource('hello.there')
		tokens = tt.allTokens()
		.checkTokens(tokens, 'ID DOT DOT ID NEWLINE')

		# tokens know their line numbers and columns and lengths
		tt.restart()
		tt.startSource('hello\nthere\n you\n ')
		tokens = tt.allTokens()
		# CC: tokens = for tok in tokens select tok if tok.which<>'NEWLINE' -- or something like that
		for i = 0 .. tokens.count
			if tokens[i].which=='NEWLINE'
				tokens.removeAt(i)
				i -= 1
		assert tokens.count==3, tokens.count
		hello = tokens[0]  # CC:  hello, there, you = tokens
		there = tokens[1]
		you = tokens[2]

		assert hello.lineNum==1
		assert hello.colNum==1
		assert hello.length==5
		assert there.lineNum==2
		assert there.colNum==1
		assert there.length==5
		assert you.lineNum==3
		assert you.colNum==2
		assert you.length==3



namespace System

	namespace IO

		class StringReader
			is fake
			inherits TextReader
			pass
